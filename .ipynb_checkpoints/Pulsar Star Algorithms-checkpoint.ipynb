{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulsar Stars-Logistic Regression, SVM & K Nearest Neighbors\n",
    "## Javier Urrecha, Raúl González y Mario Aviles\n",
    "### 21 de noviembre, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definiendo funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto punto\n",
    "$$θ=w^TX$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta(w, x):\n",
    "    \n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "        returns:\n",
    "            theta: [np_array] a vector of the inner product of w and x with dimensions (1xm)\n",
    "    \"\"\"\n",
    "        \n",
    "    return w.T.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hipótesis regresión logística\n",
    "$$h_w(x)=g(w^TX)=\\frac{1}{1+e^{-w^TX}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"\n",
    "        params:\n",
    "            z: [np_array] a vector of the inner product of w and x with dimensions (1xm)\n",
    "        returns:\n",
    "            sigmoid: [np_array] a vector of the estimations performed by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    return (1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de costo regresión logística\n",
    "$$J(w)=-\\sum_{i=1}^{m}y^{(i)}\\log h_w(x^{(i)}) +(1-y^{(i)})\\log(1-h_w(x^{(i)}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(w, x, y):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of target variables with dimensions (mx1), \n",
    "                where m represents the number of target variables\n",
    "        returns:\n",
    "            J: [double] the loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    z=theta(w,x)\n",
    "    h=sigmoid(z)\n",
    "    return (1/(x.shape[1]))*np.sum((-y * np.log(h) - (1 - y) * np.log(1 - h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de la función de costo regresión logística\n",
    "$$\\frac{\\delta }{\\delta w_j}J(w)=(y-h_w(x))x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(w, x, y):\n",
    "    \n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of target variables with dimensions (mx1), \n",
    "                where m represents the number of target variables\n",
    "        returns:\n",
    "            dJ: [double] the derivative of the loss function\n",
    "    \"\"\"\n",
    "\n",
    "    z=theta(w,x)\n",
    "    e = sigmoid(z).T - y\n",
    "    \n",
    "    return (1 / (x.shape[1]))*(np.dot(x, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso por gradiente-LMS\n",
    "$$w:= w-\\alpha \\nabla_wJ(w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizar_LMS(x_train, y_train, x_test, y_test, num_iter, alpha, w=None):\n",
    "\n",
    "    \"\"\"\n",
    "        We calculate gradient descent for minimizing the MSE to obtain the best linear hypothesis.\n",
    "            params:\n",
    "                x_train: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                    where n represents the number of feature variables and m the number of training examples\n",
    "                x_test: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                    where n represents the number of feature variables and m the number of validating examples\n",
    "                y_train: [np_array] a vector of target variables with dimensions (mx1), \n",
    "                    where m represents the number of training target variables\n",
    "                y_test: [np_array] a vector of target variables with dimensions (mx1), \n",
    "                    where m represents the number of validating target variables\n",
    "                num_iter: [int] an integer indicating the number of iterations of the Gradient Descent algorithm\n",
    "                alpha: [double] learning rate constant specifying the magnitude update step\n",
    "                w: [np_array] vector that contains the initial weights to start optimzing the model with dimensions (n x 1)\n",
    "\n",
    "            return:\n",
    "                J_train: [np_array] a vector (num_iter x 1) containing all cost function evaluations during training\n",
    "                J_test: [np_array] a vector (num_iter x 1) containing all cost function evaluations during training\n",
    "                w: [np_array] a vector of the final optimized weights with dimensions (nx1)\n",
    "    \"\"\"\n",
    "\n",
    "    if w is None:\n",
    "        # Inicializamos los pesos aleatoriamente\n",
    "        w = np.random.randn(x_train.shape[0], 1)\n",
    "\n",
    "    # se generan los vectores\n",
    "    it = np.arange(0, num_iter)\n",
    "    J_train = np.zeros(num_iter)\n",
    "    J_test = np.zeros(num_iter)\n",
    "\n",
    "    # Se optimiza el modelo por el numero de iteraciones\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Actualizamos los pesos\n",
    "        w = w - alpha * dJ(w, x_train, y_train)\n",
    "\n",
    "        # Guardamos los costo\n",
    "        J_train[i] = J(w, x_train, y_train)\n",
    "\n",
    "        J_test[i] = J(w, x_test, y_test)\n",
    "\n",
    "    return w, J_train, J_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba(w,x):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "        returns:\n",
    "            probability: [np_array] a vector with the probabilities of belonging to class 1, with dimensions (1xm)\n",
    "    \"\"\"\n",
    "    proba=np.zeros(x.shape[1])\n",
    "    n=0\n",
    "    for i in range(0,x.shape[1]):\n",
    "        y_hat = sigmoid(theta(w, x[:,i]))\n",
    "        proba[n]=y_hat\n",
    "        n+=1\n",
    "\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(w,x_test,porcentaje):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a matrix of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "        returns:\n",
    "            prediction: [np_array] a vector with the predicted classes, with dimensions (1xm)\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    for i in range(0,x_test.shape[1]):\n",
    "        y_hat = sigmoid(theta(w, x_test[:,i]))           \n",
    "        if y_hat>porcentaje:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(arr,min,max):\n",
    "    \n",
    "    \"\"\"\n",
    "        params:\n",
    "            arr: [np_array] a vector of training/validating examples with dimensions (mx1), \n",
    "            where m represents the number training or validating examples\n",
    "            min: [double] the minimum value of the array\n",
    "            max: [double] the maximum value of the array  \n",
    "        returns:\n",
    "            normalization: [np_array] the normalized vector (mx1)\n",
    "    \"\"\"\n",
    "    return (arr - min) / (max - min)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importando datos, balanceando y normalizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean of the integrated profile</th>\n",
       "      <th>Standard deviation of the integrated profile</th>\n",
       "      <th>Excess kurtosis of the integrated profile</th>\n",
       "      <th>Skewness of the integrated profile</th>\n",
       "      <th>Mean of the DM-SNR curve</th>\n",
       "      <th>Standard deviation of the DM-SNR curve</th>\n",
       "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
       "      <th>Skewness of the DM-SNR curve</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean of the integrated profile  \\\n",
       "0                       140.562500   \n",
       "1                       102.507812   \n",
       "2                       103.015625   \n",
       "3                       136.750000   \n",
       "4                        88.726562   \n",
       "\n",
       "    Standard deviation of the integrated profile  \\\n",
       "0                                      55.683782   \n",
       "1                                      58.882430   \n",
       "2                                      39.341649   \n",
       "3                                      57.178449   \n",
       "4                                      40.672225   \n",
       "\n",
       "    Excess kurtosis of the integrated profile  \\\n",
       "0                                   -0.234571   \n",
       "1                                    0.465318   \n",
       "2                                    0.323328   \n",
       "3                                   -0.068415   \n",
       "4                                    0.600866   \n",
       "\n",
       "    Skewness of the integrated profile   Mean of the DM-SNR curve  \\\n",
       "0                            -0.699648                   3.199833   \n",
       "1                            -0.515088                   1.677258   \n",
       "2                             1.051164                   3.121237   \n",
       "3                            -0.636238                   3.642977   \n",
       "4                             1.123492                   1.178930   \n",
       "\n",
       "    Standard deviation of the DM-SNR curve  \\\n",
       "0                                19.110426   \n",
       "1                                14.860146   \n",
       "2                                21.744669   \n",
       "3                                20.959280   \n",
       "4                                11.468720   \n",
       "\n",
       "    Excess kurtosis of the DM-SNR curve   Skewness of the DM-SNR curve  \\\n",
       "0                              7.975532                      74.242225   \n",
       "1                             10.576487                     127.393580   \n",
       "2                              7.735822                      63.171909   \n",
       "3                              6.896499                      53.593661   \n",
       "4                             14.269573                     252.567306   \n",
       "\n",
       "   target_class  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicamos la dirección y nombre del archivo y se imprime la cabecera\n",
    "data = pd.read_csv('HTRU_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17898, 9)\n"
     ]
    }
   ],
   "source": [
    "#Se imprimen las dimensiones del conjunto de datos\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12528, 9)\n",
      "(5370, 9)\n"
     ]
    }
   ],
   "source": [
    "#Se utiliza la función train_test_split para separar los datos en 70% de entrenamiento y 30% de validación\n",
    "data_train , data_test = train_test_split(data,test_size = .3,random_state = 100)\n",
    "\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12528, 8)\n",
      "(12528, 1)\n",
      "(5370, 8)\n",
      "(5370, 1)\n"
     ]
    }
   ],
   "source": [
    "#Se seleccionan las primeras ocho columnas para las características y la última columna para la clase,\n",
    "#tanto en entrenamiento como en validación\n",
    "X_train = data_train[[x for x in data_train.columns if x not in [\"target_class\"]]]\n",
    "Y_train = data_train[[\"target_class\"]]\n",
    "X_test  = data_test[[x for x in data_test.columns if x not in [\"target_class\"]]]\n",
    "Y_test  = data_test[[\"target_class\"]]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 11385\n",
      "Class 1: 1143\n",
      "Proportion: 9.96 : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Se cuentan los datos de la clase para ver si el conjunto de entrenamiento de la clase está balanceado\n",
    "target_count = Y_train.target_class.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "target_count.plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12528, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "(12528, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Se convierten los conjuntos de entrenamiento en vectores\n",
    "X_train_list = X_train.values.tolist()\n",
    "X_train = np.array(X_train_list, dtype='float64')\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "\n",
    "Y_train_list = Y_train.values.tolist()\n",
    "Y_train = np.array(Y_train_list, dtype='float64')\n",
    "print(Y_train.shape)\n",
    "print(type(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\73830\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Se utiliza la función SMOTEEEN para balancear los datos de entrenamiento\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_train_res, Y_train_res = smote_enn.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 10014\n",
      "Class 1: 10835\n",
      "Proportion: 0.92 : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Se verifica que los datos de entrenamiento estén balanceados\n",
    "Y_train_res=pd.DataFrame(Y_train_res, columns=['target_class'])\n",
    "target_count_res = Y_train_res.target_class.value_counts()\n",
    "print('Class 0:', target_count_res[0])\n",
    "print('Class 1:', target_count_res[1])\n",
    "print('Proportion:', round(target_count_res[0] / target_count_res[1], 2), ': 1')\n",
    "\n",
    "target_count_res.plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20849, 1)\n",
      "(20849, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Se imprime el tipo y dimensiones de los conjuntos\n",
    "print(Y_train_res.shape)\n",
    "print(X_train_res.shape)\n",
    "print(type(Y_train_res))\n",
    "print(type(X_train_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20849, 8)\n",
      "(20849, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Se convierten los conjuntos en vectores\n",
    "Y_train_list = Y_train_res.values.tolist()\n",
    "\n",
    "y_train = np.array(Y_train_list, dtype='float64')\n",
    "\n",
    "x_train=X_train_res\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5370, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "(5370, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Se convierten los conjuntos de validación en vectores\n",
    "X_test_list = X_test.values.tolist()\n",
    "X_test = np.array(X_test_list, dtype='float64')\n",
    "x_test=X_test\n",
    "print(x_test.shape)\n",
    "print(type(x_test))\n",
    "\n",
    "\n",
    "Y_test_list = Y_test.values.tolist()\n",
    "Y_test = np.array(Y_test_list, dtype='float64')\n",
    "y_test=Y_test\n",
    "print(y_test.shape)\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se normalizan los conjuntos de datos de características\n",
    "x_train_norm = np.zeros((x_train.shape[0],x_train.shape[1]))\n",
    "x_test_norm = np.zeros((x_test.shape[0],x_test.shape[1]))\n",
    "\n",
    "for i in range (0,x_train.shape[1]):\n",
    "    x_train_norm[:,i]=norm(x_train[:,i],min(x_train[:,i]), max(x_train[:,i]))\n",
    "    \n",
    "for i in range (0,x_test.shape[1]):\n",
    "    x_test_norm[:,i]=norm(x_test[:,i],min(x_train[:,i]), max(x_train[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58307892 0.28067169 0.21173358 ... 0.59931604 0.15051058 0.00538666]\n",
      " [0.55426373 0.19049382 0.20553339 ... 0.0490563  0.40856306 0.16046392]\n",
      " [0.5369077  0.23974651 0.24268133 ... 0.0944188  0.32870794 0.08365848]\n",
      " ...\n",
      " [0.30431934 0.22046213 0.4154943  ... 0.33760671 0.19023258 0.01509734]\n",
      " [0.08556533 0.20065443 0.64736297 ... 0.65557516 0.08399583 0.00078449]\n",
      " [0.51801376 0.30679789 0.24278695 ... 0.65794871 0.11994746 0.00189978]]\n",
      "[[0.58123876 0.14939957 0.23421148 ... 0.05359629 0.37712398 0.14024695]\n",
      " [0.53448204 0.29425281 0.22322145 ... 0.14229973 0.28370677 0.05190835]\n",
      " [0.58370624 0.43302787 0.19818329 ... 0.52204571 0.0497964  0.00147159]\n",
      " ...\n",
      " [0.37560119 0.25727772 0.27537442 ... 0.15452032 0.28973011 0.05426537]\n",
      " [0.50198653 0.29075309 0.25006323 ... 0.34093246 0.18562476 0.0132561 ]\n",
      " [0.4934549  0.2494336  0.22766679 ... 0.0555486  0.42695732 0.1622515 ]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_norm)\n",
    "print(x_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creando los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 20849)\n",
      "(9, 5370)\n"
     ]
    }
   ],
   "source": [
    "#Se añade la unidad de sesgo\n",
    "x_train_norm_b = np.insert(x_train_norm, 0, 1, axis=1)\n",
    "x_train_norm_b = x_train_norm_b.T\n",
    "\n",
    "print(x_train_norm_b.shape)\n",
    "\n",
    "x_test_norm_b = np.insert(x_test_norm, 0, 1, axis=1)\n",
    "x_test_norm_b = x_test_norm_b.T\n",
    "print(x_test_norm_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1)\n",
      "[[ 1.e+00]\n",
      " [-1.e-01]\n",
      " [-1.e-01]\n",
      " [ 3.e+00]\n",
      " [-3.e-01]\n",
      " [-3.e-02]\n",
      " [ 3.e-02]\n",
      " [-3.e-02]\n",
      " [-1.e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Se asigna un vector w arbitrario\n",
    "w0=1\n",
    "w1 = -0.1\n",
    "w2 = -0.1\n",
    "w3 = 3\n",
    "w4 = -0.3\n",
    "w5 = -0.03\n",
    "w6 = 0.03\n",
    "w7 = -0.03\n",
    "w8 = -0.0001\n",
    "\n",
    "w = np.array([w0,w1, w2, w3, w4, w5, w6, w7, w8], dtype='float64')\n",
    "w = w.reshape((w.shape[0], 1))\n",
    "print(w.shape)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se corre el descenso por gradiente para encontrar los mejores pesos w y el costo de entrenamiento y validación\n",
    "alpha=0.001\n",
    "num_iter=50\n",
    "w, j_train, j_test = optimizar_LMS(x_train_norm_b, y_train, x_test_norm_b, y_test, num_iter, alpha, w)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos el costo\n",
    "it = np.linspace(0, num_iter, num_iter)\n",
    "plt.figure()\n",
    "plt.plot(it, j_train, \"k\", linewidth=3, label=\"Costo entrenamiento\")\n",
    "plt.plot(it, j_test, \"r\", linewidth=3, label=\"Costo validacion\")\n",
    "plt.title(\"Costo J(w)\")\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo') \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo con umbral de 0.85\n",
    "porcentaje=0.85\n",
    "#Se calculan las predicciones de clases\n",
    "val_preds=classify(w,x_test_norm_b,porcentaje)\n",
    "print(type(val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Las predicciones se convierten en vectores\n",
    "val_preds = np.array(val_preds, dtype='float64')\n",
    "val_preds = val_preds.reshape(val_preds.shape[0], 1)\n",
    "print(val_preds.shape)\n",
    "for i in val_preds:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se calcula la matriz de confusión\n",
    "confusion = confusion_matrix(y_test, val_preds)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion[0,0]+confusion[1,1])/(confusion[0,0]+confusion[0,1]+confusion[1,0]+confusion[1,1]))\n",
    "precision=(confusion[1,1]/(confusion[1,1]+confusion[0,1]))\n",
    "sensitivity=(confusion[1,1]/(confusion[1,1]+confusion[1,0]))\n",
    "specificity=(confusion[0,0]/(confusion[0,0]+confusion[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se corre el algorítmo con diferentes umbrales\n",
    "min_umbral=0.8\n",
    "max_umbral=0.95\n",
    "umbrals=101\n",
    "umbral = np.linspace(min_umbral, max_umbral, umbrals)\n",
    "logistic_accuracy=np.zeros(umbral.size-1)\n",
    "logistic_precision=np.zeros(umbral.size-1)\n",
    "logistic_sensitivity=np.zeros(umbral.size-1)\n",
    "logistic_specificity=np.zeros(umbral.size-1)\n",
    "logistic_f1score=np.zeros(umbral.size-1)\n",
    "n=0\n",
    "\n",
    "for i in range(1,101):\n",
    "    porcentaje=umbral[i]\n",
    "    val_preds=classify(w,x_test_norm_b,porcentaje)\n",
    "    confusion_logistic = confusion_matrix(y_test, val_preds)\n",
    "    \n",
    "    \n",
    "    logistic_accuracy[n]=((confusion_logistic[0,0]+confusion_logistic[1,1])/(confusion_logistic[0,0]+confusion_logistic[0,1]+confusion_logistic[1,0]+confusion_logistic[1,1]))\n",
    "    logistic_precision[n]=(confusion_logistic[1,1]/(confusion_logistic[1,1]+confusion_logistic[0,1]))\n",
    "    logistic_sensitivity[n]=(confusion_logistic[1,1]/(confusion_logistic[1,1]+confusion_logistic[1,0]))\n",
    "    logistic_specificity[n]=(confusion_logistic[0,0]/(confusion_logistic[0,0]+confusion_logistic[0,1]))\n",
    "    logistic_f1score[n]=(2*(logistic_precision[n]*logistic_sensitivity[n])/(logistic_precision[n]+logistic_sensitivity[n]))\n",
    "    n+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_sensitivity)\n",
    "print(1-logistic_specificity)\n",
    "print(max(logistic_accuracy))\n",
    "print(logistic_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se obtiene el umbral óptimo\n",
    "optimal_logistic_idx=np.argmax(logistic_sensitivity-(1-logistic_specificity))\n",
    "print('optimal threshold index: ',optimal_logistic_idx)\n",
    "\n",
    "optimal_logistic_threshold=umbral[optimal_logistic_idx]\n",
    "print('optimal threshold: ',optimal_logistic_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos curva ROC con lo datos obtenidos\n",
    "plt.figure()\n",
    "plt.plot((1-logistic_specificity),logistic_sensitivity,\"r\", linewidth=3)\n",
    "plt.scatter((1-logistic_specificity[optimal_logistic_idx]),logistic_sensitivity[optimal_logistic_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % umbral[optimal_logistic_idx]))\n",
    "plt.title(\"ROC Curve\\nLogistic Regression\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando el mejor umbral\n",
    "porcentaje=umbral[optimal_logistic_idx]\n",
    "#Se calculan las predicciones de clases\n",
    "val_preds=classify(w,x_test_norm_b,porcentaje)\n",
    "\n",
    "#Las predicciones se convierten en vectores\n",
    "val_preds = np.array(val_preds, dtype='float64')\n",
    "val_preds = val_preds.reshape(val_preds.shape[0], 1)\n",
    "print(val_preds.shape)\n",
    "\n",
    "#Se calcula la matriz de confusión\n",
    "confusion_best_logistic= confusion_matrix(y_test, val_preds)\n",
    "print(confusion_best_logistic)\n",
    "\n",
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_best_logistic[0,0]+confusion_best_logistic[1,1])/(confusion_best_logistic[0,0]+confusion_best_logistic[0,1]+confusion_best_logistic[1,0]+confusion_best_logistic[1,1]))\n",
    "precision=(confusion_best_logistic[1,1]/(confusion_best_logistic[1,1]+confusion_best_logistic[0,1]))\n",
    "sensitivity=(confusion_best_logistic[1,1]/(confusion_best_logistic[1,1]+confusion_best_logistic[1,0]))\n",
    "specificity=(confusion_best_logistic[0,0]/(confusion_best_logistic[0,0]+confusion_best_logistic[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística con librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la probabilidad de la clase 1 para cada dato de validación\n",
    "probs=proba(w,x_test_norm_b)\n",
    "probs = probs.reshape((probs.shape[0], 1))\n",
    "print(probs.shape)\n",
    "for i in probs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el false positve rate y el true positive rate con librerías\n",
    "fpr_logistic, tpr_logistic, thresholds_logistic = metrics.roc_curve(y_test, probs)\n",
    "\n",
    "#Calculamos el área bajo la curva ROC utilizando librerías\n",
    "auc_logistic=skm.roc_auc_score(y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se obtiene el umbral óptimo\n",
    "optimal_thresholds_logistic_idx=np.argmax(tpr_logistic-fpr_logistic)\n",
    "print('optimal threshold index: ',optimal_thresholds_logistic_idx)\n",
    "\n",
    "optimal_thresholds_logistic=thresholds_logistic[optimal_thresholds_logistic_idx]\n",
    "print('optimal threshold: ',optimal_thresholds_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos la curva ROC utilizando librerías\n",
    "plt.figure()\n",
    "plt.plot(fpr_logistic, tpr_logistic,\"r\", linewidth=3, label='AUC {}'.format('%.4f' % auc_logistic))\n",
    "plt.scatter((fpr_logistic[optimal_thresholds_logistic_idx]),tpr_logistic[optimal_thresholds_logistic_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % thresholds_logistic[optimal_thresholds_logistic_idx]))\n",
    "plt.title(\"ROC Curve\\nLogistic Regression\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando el mejor umbral obtenido con librerías\n",
    "porcentaje=thresholds_logistic[optimal_thresholds_logistic_idx]\n",
    "#Se calculan las predicciones de clases\n",
    "val_preds=classify(w,x_test_norm_b,porcentaje)\n",
    "\n",
    "#Las predicciones se convierten en vectores\n",
    "val_preds = np.array(val_preds, dtype='float64')\n",
    "val_preds = val_preds.reshape(val_preds.shape[0], 1)\n",
    "print(val_preds.shape)\n",
    "\n",
    "#Se calcula la matriz de confusión\n",
    "confusion_best_logistic= confusion_matrix(y_test, val_preds)\n",
    "print(confusion_best_logistic)\n",
    "\n",
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_best_logistic[0,0]+confusion_best_logistic[1,1])/(confusion_best_logistic[0,0]+confusion_best_logistic[0,1]+confusion_best_logistic[1,0]+confusion_best_logistic[1,1]))\n",
    "precision=(confusion_best_logistic[1,1]/(confusion_best_logistic[1,1]+confusion_best_logistic[0,1]))\n",
    "sensitivity=(confusion_best_logistic[1,1]/(confusion_best_logistic[1,1]+confusion_best_logistic[1,0]))\n",
    "specificity=(confusion_best_logistic[0,0]/(confusion_best_logistic[0,0]+confusion_best_logistic[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.ravel(y_train,order='C')\n",
    "print(type(y_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el kernel primero como lineal y entrenamos el modelo\n",
    "svclassifier = SVC(kernel='linear', probability=True)\n",
    "svclassifier.fit(x_train_norm, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontramos las predicciones\n",
    "y_svm_pred = svclassifier.predict(x_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la matrix de confusión\n",
    "confusion_svm = confusion_matrix(y_test,y_svm_pred)\n",
    "print(confusion_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_svm[0,0]+confusion_svm[1,1])/(confusion_svm[0,0]+confusion_svm[0,1]+confusion_svm[1,0]+confusion_svm[1,1]))\n",
    "precision=(confusion_svm[1,1]/(confusion_svm[1,1]+confusion_svm[0,1]))\n",
    "sensitivity=(confusion_svm[1,1]/(confusion_svm[1,1]+confusion_svm[1,0]))\n",
    "specificity=(confusion_svm[0,0]/(confusion_svm[0,0]+confusion_svm[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Encontramos la probabilidad de las perdicciones que sea clase 1\n",
    "y_svm_pred_proba = svclassifier.predict_proba(x_test_norm)[:,1]\n",
    "\n",
    "for i in y_svm_pred_proba:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el false positve rate y el true positive rate con librerías\n",
    "fpr_svm, tpr_svm, thresholds_svm = metrics.roc_curve(y_test, y_svm_pred_proba)\n",
    "\n",
    "#Calculamos el área bajo la curva ROC utilizando librerías\n",
    "auc_svm=skm.roc_auc_score(y_test,y_svm_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se obtiene el umbral óptimo\n",
    "optimal_thresholds_svm_idx=np.argmax(tpr_svm-fpr_svm)\n",
    "print('optimal threshold index: ',optimal_thresholds_svm_idx)\n",
    "\n",
    "optimal_thresholds_svm=thresholds_svm[optimal_thresholds_svm_idx]\n",
    "print('optimal threshold: ',optimal_thresholds_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos la curva ROC utilizando librerías\n",
    "plt.figure()\n",
    "plt.plot(fpr_svm, tpr_svm,\"r\", linewidth=3, label='AUC {}'.format('%.4f' % auc_svm))\n",
    "plt.scatter((fpr_svm[optimal_thresholds_svm_idx]),tpr_svm[optimal_thresholds_svm_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % thresholds_svm[optimal_thresholds_svm_idx]))\n",
    "plt.title(\"ROC Curve\\nSVM Linear Kernel\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el kernel radial\n",
    "svclassifier = SVC(kernel='rbf',probability=True)\n",
    "svclassifier.fit(x_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontramos las predicciones\n",
    "y_kernelsvm_pred = svclassifier.predict(x_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontramos la probabilidad de las perdicciones que sea clase 1\n",
    "y_kernelsvm_pred_proba = svclassifier.predict_proba(x_test_norm)[:,1]\n",
    "\n",
    "for i in y_kernelsvm_pred_proba:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos el false positve rate y el true positive rate con librerías\n",
    "fpr_kernelsvm, tpr_kernelsvm, thresholds_kernelsvm = metrics.roc_curve(y_test, y_kernelsvm_pred_proba)\n",
    "\n",
    "#Calculamos el área bajo la curva ROC utilizando librerías\n",
    "auc_kernelsvm=skm.roc_auc_score(y_test,y_kernelsvm_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se obtiene el umbral óptimo\n",
    "optimal_thresholds_kernelsvm_idx=np.argmax(tpr_kernelsvm-fpr_kernelsvm)\n",
    "print('optimal threshold index: ',optimal_thresholds_kernelsvm_idx)\n",
    "\n",
    "optimal_thresholds_kernelsvm=thresholds_kernelsvm[optimal_thresholds_kernelsvm_idx]\n",
    "print('optimal threshold: ',optimal_thresholds_kernelsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos la curva ROC utilizando librerías\n",
    "plt.figure()\n",
    "plt.plot(fpr_kernelsvm, tpr_kernelsvm,\"r\", linewidth=3, label='AUC {}'.format('%.4f' % auc_kernelsvm))\n",
    "plt.scatter((fpr_kernelsvm[optimal_thresholds_kernelsvm_idx]),tpr_kernelsvm[optimal_thresholds_kernelsvm_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % thresholds_kernelsvm[optimal_thresholds_kernelsvm_idx]))\n",
    "plt.title(\"ROC Curve\\nSVM Radial Kernel\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la matrix de confusión\n",
    "confusion_kernelsvm = confusion_matrix(y_test,y_kernelsvm_pred)\n",
    "print(confusion_kernelsvm)\n",
    "\n",
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_kernelsvm[0,0]+confusion_kernelsvm[1,1])/(confusion_kernelsvm[0,0]+confusion_kernelsvm[0,1]+confusion_kernelsvm[1,0]+confusion_svm[1,1]))\n",
    "precision=(confusion_kernelsvm[1,1]/(confusion_kernelsvm[1,1]+confusion_kernelsvm[0,1]))\n",
    "sensitivity=(confusion_kernelsvm[1,1]/(confusion_kernelsvm[1,1]+confusion_kernelsvm[1,0]))\n",
    "specificity=(confusion_kernelsvm[0,0]/(confusion_kernelsvm[0,0]+confusion_kernelsvm[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero entrenamos el modelo con un valor de k=5\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN.fit(x_train_norm,y_train)\n",
    "\n",
    "#Encontramos las predicciones para KNN con 5 neighbors\n",
    "y_KNN_pred = KNN.predict(x_test_norm)\n",
    "#Encontramos la probabilidad de las perdicciones que sea clase 1\n",
    "y_KNN_pred_proba = KNN.predict_proba(x_test_norm)[:,1]\n",
    "\n",
    "#Calculamos la matrix de confusión\n",
    "confusion_KNN = confusion_matrix(y_test,y_KNN_pred)\n",
    "print(confusion_KNN)\n",
    "\n",
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_KNN[0,0]+confusion_KNN[1,1])/(confusion_KNN[0,0]+confusion_KNN[0,1]+confusion_KNN[1,0]+confusion_KNN[1,1]))\n",
    "precision=(confusion_KNN[1,1]/(confusion_KNN[1,1]+confusion_KNN[0,1]))\n",
    "sensitivity=(confusion_KNN[1,1]/(confusion_KNN[1,1]+confusion_KNN[1,0]))\n",
    "specificity=(confusion_KNN[0,0]/(confusion_KNN[0,0]+confusion_KNN[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)\n",
    "\n",
    "#Obtenemos el false positve rate y el true positive rate con librerías\n",
    "fpr_KNN, tpr_KNN, thresholds_KNN = metrics.roc_curve(y_test, y_KNN_pred_proba)\n",
    "\n",
    "#Calculamos el área bajo la curva ROC utilizando librerías\n",
    "auc_KNN=skm.roc_auc_score(y_test,y_KNN_pred_proba)\n",
    "\n",
    "#Encontramos el umbral óptimo\n",
    "optimal_thresholds_KNN_idx=np.argmax(tpr_KNN-fpr_KNN)\n",
    "print('optimal threshold index: ',optimal_thresholds_KNN_idx)\n",
    "\n",
    "optimal_thresholds_KNN=thresholds_KNN[optimal_thresholds_KNN_idx]\n",
    "print('optimal threshold: ',optimal_thresholds_KNN)\n",
    "\n",
    "#Graficamos la curva ROC utilizando librerías\n",
    "plt.figure()\n",
    "plt.plot(fpr_KNN, tpr_KNN,\"r\", linewidth=3, label='AUC {}'.format('%.4f' % auc_KNN))\n",
    "plt.scatter((fpr_KNN[optimal_thresholds_KNN_idx]),tpr_KNN[optimal_thresholds_KNN_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % thresholds_KNN[optimal_thresholds_KNN_idx]))\n",
    "plt.title(\"ROC Curve\\nK Nearest Neighbors\\nK=5\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontramos el valor de k que maximice la el f1score\n",
    "\n",
    "min_k=1\n",
    "max_k=100\n",
    "k=np.linspace(min_k, max_k, max_k)\n",
    "KNN_precision=np.zeros(max_k)\n",
    "KNN_sensitivity=np.zeros(max_k)\n",
    "KNN_f1score=np.zeros(max_k)\n",
    "n=0\n",
    "\n",
    "for i in range (min_k,max_k+1):\n",
    "    KNN = KNeighborsClassifier(n_neighbors=i)\n",
    "    KNN.fit(x_train_norm,y_train)\n",
    "    y_KNN_pred = KNN.predict(x_test_norm)\n",
    "    confusion_KNN = confusion_matrix(y_test,y_KNN_pred)\n",
    "    KNN_precision[n]=(confusion_KNN[1,1]/(confusion_KNN[1,1]+confusion_KNN[0,1]))\n",
    "    KNN_sensitivity[n]=(confusion_KNN[1,1]/(confusion_KNN[1,1]+confusion_KNN[1,0]))\n",
    "    KNN_f1score[n]=2*(KNN_precision[n]*KNN_sensitivity[n])/(KNN_precision[n]+KNN_sensitivity[n])\n",
    "    n+=1\n",
    "    \n",
    "print(KNN_f1score)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprime el mejor número de k\n",
    "k=np.array(k, dtype='int64')\n",
    "print('Mejor valor de K: ', k[KNN_f1score.argmax()])\n",
    "\n",
    "#Graficamos el f1score en función del valor de k\n",
    "plt.figure()\n",
    "plt.scatter(k, KNN_f1score, alpha = 1, color='r')\n",
    "plt.scatter(k[KNN_f1score.argmax()], KNN_f1score[KNN_f1score.argmax()], alpha = 1, color='b', label='Best K {}'.format('%d' % k[KNN_f1score.argmax()]))\n",
    "plt.title(\"f1score in function of K\")\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('f1score') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el modelo con el mejor valor de K\n",
    "KNN = KNeighborsClassifier(n_neighbors=k[KNN_f1score.argmax()])\n",
    "KNN.fit(x_train_norm,y_train)\n",
    "\n",
    "#Encontramos las predicciones y probabilidad de clase 1 para KNN con el valor de k que máximiza la precisión\n",
    "y_KNN_pred = KNN.predict(x_test_norm)\n",
    "y_KNN_pred_proba = KNN.predict_proba(x_test_norm)[:,1]\n",
    "\n",
    "#Calculamos la matrix de confusión\n",
    "confusion_KNN = confusion_matrix(y_test,y_KNN_pred)\n",
    "print(confusion_KNN)\n",
    "\n",
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_KNN[0,0]+confusion_KNN[1,1])/(confusion_KNN[0,0]+confusion_KNN[0,1]+confusion_KNN[1,0]+confusion_KNN[1,1]))\n",
    "precision=(confusion_KNN[1,1]/(confusion_KNN[1,1]+confusion_KNN[0,1]))\n",
    "sensitivity=(confusion_KNN[1,1]/(confusion_KNN[1,1]+confusion_KNN[1,0]))\n",
    "specificity=(confusion_KNN[0,0]/(confusion_KNN[0,0]+confusion_KNN[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)\n",
    "\n",
    "#Obtenemos el false positve rate y el true positive rate con librerías\n",
    "fpr_KNN, tpr_KNN, thresholds_KNN = metrics.roc_curve(y_test, y_KNN_pred_proba)\n",
    "\n",
    "#Calculamos el área bajo la curva ROC utilizando librerías\n",
    "auc_KNN=skm.roc_auc_score(y_test,y_KNN_pred_proba)\n",
    "\n",
    "#Encontramos el umbral óptimo\n",
    "optimal_thresholds_KNN_idx=np.argmax(tpr_KNN-fpr_KNN)\n",
    "print('optimal threshold index: ',optimal_thresholds_KNN_idx)\n",
    "\n",
    "optimal_thresholds_KNN=thresholds_KNN[optimal_thresholds_KNN_idx]\n",
    "print('optimal threshold: ',optimal_thresholds_KNN)\n",
    "\n",
    "#Graficamos la curva ROC utilizando librerías\n",
    "plt.figure()\n",
    "plt.plot(fpr_KNN, tpr_KNN,\"r\", linewidth=3, label='AUC {}'.format('%.4f' % auc_KNN))\n",
    "plt.scatter((fpr_KNN[optimal_thresholds_KNN_idx]),tpr_KNN[optimal_thresholds_KNN_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % thresholds_KNN[optimal_thresholds_KNN_idx]))\n",
    "plt.title(\"ROC Curve\\nK Nearest Neighbors\\nK=42\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea el modeloo\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(4, input_shape= (8,), activation ='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= 'sgd', metrics = ['accuracy'])\n",
    "class_weight = {0 : 1., 1 : 2.}\n",
    "H = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    batch_size = 128,\n",
    "                   epochs = 200,\n",
    "                   class_weight=class_weight,\n",
    "                   verbose = 0,\n",
    "                   validation_data = (X_test, Y_test))\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "#Se hacen las predicciones del modelo\n",
    "predictions = model.predict(X_test, batch_size=128)\n",
    "predictions[predictions < 0.5] = 0\n",
    "predictions[predictions >= 0.5] = 1\n",
    "report = classification_report(Y_test, predictions, \n",
    "                               target_names=['Non-pulsar Star', 'Pulsar Star'])\n",
    "\n",
    "print('Accuracy = {:.7f}'.format(scores[1]))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se obtienen las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se calcula la matriz de confusión\n",
    "confusion_best_logistic= confusion_matrix(Y_test, predictions)\n",
    "print(confusion_best_logistic)\n",
    "\n",
    "#Se calculan las métricas de evaluación\n",
    "accuracy=((confusion_best_logistic[0,0]+confusion_best_logistic[1,1])/(confusion_best_logistic[0,0]+confusion_best_logistic[0,1]+confusion_best_logistic[1,0]+confusion_best_logistic[1,1]))\n",
    "precision=(confusion_best_logistic[1,1]/(confusion_best_logistic[1,1]+confusion_best_logistic[0,1]))\n",
    "sensitivity=(confusion_best_logistic[1,1]/(confusion_best_logistic[1,1]+confusion_best_logistic[1,0]))\n",
    "specificity=(confusion_best_logistic[0,0]/(confusion_best_logistic[0,0]+confusion_best_logistic[0,1]))\n",
    "f1score=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "\n",
    "print('acurracy:',accuracy)\n",
    "print('precision:',precision)\n",
    "print('sensitivity:',sensitivity)\n",
    "print('specificity:',specificity)\n",
    "print('f1score:',f1score)\n",
    "\n",
    "#Obtenemos el false positve rate y el true positive rate con librerías\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "\n",
    "#Calculamos el área bajo la curva ROC utilizando librerías\n",
    "auc=skm.roc_auc_score(y_test, predictions)\n",
    "\n",
    "#Encontramos el umbral óptimo\n",
    "optimal_thresholds_idx=np.argmax(tpr-fpr)\n",
    "print('optimal threshold index: ',optimal_thresholds_idx)\n",
    "\n",
    "optimal_thresholds=thresholds[optimal_thresholds_idx]\n",
    "print('optimal threshold: ',optimal_thresholds)\n",
    "\n",
    "#Graficamos la curva ROC utilizando librerías\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr,\"r\", linewidth=3, label='AUC {}'.format('%.4f' % auc))\n",
    "plt.scatter((fpr[optimal_thresholds_idx]),tpr[optimal_thresholds_idx], alpha = 1, color='b', label='Best Threshold {}'.format('%.4f' % thresholds[optimal_thresholds_idx]))\n",
    "plt.title(\"ROC Curve\\nNeural networks\\n\")\n",
    "plt.xlabel('1-Specificity')\n",
    "plt.ylabel('Sensitivity') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
